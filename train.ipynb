{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jaysun/miniforge3/envs/mlp/lib/python3.8/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/jaysun/miniforge3/envs/mlp/lib/python3.8/site-packages (from torch) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"torchmetrics>=0.7, <0.12\" \"seaborn\" \"ipython[notebook]>=8.0.0, <8.9.0\" \"pytorch-lightning>=1.4, <1.9\" \"torchmetrics >=0.11.0\" \"setuptools==65.6.3\" \"pandas\" \"torchvision\" \"torch>=1.8.1, <1.14.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertTokenizerFast, AutoModel, GPT2Tokenizer, BertTokenizer, BertModel, BertLMHeadModel\n",
    "import json\n",
    "from torch import nn\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import random\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jaysun/Desktop/Python_Projects/GPTRLFinetune'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd = os.getcwd()\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(os.path.join(pwd, 'GPT-2/GPT2_finetune_1'))\n",
    "bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "bert_lm = BertLMHeadModel.from_pretrained('bert-base-chinese')\n",
    "# tokenizer = BertTokenizer(vocab_file = './vocab_small.txt')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 21128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_lm(input_ids = torch.tensor([[1,2,3,54], [3, 5, 5, 3]]), attention_mask = torch.tensor([[1, 1, 0, 1], [1, 1, 0, 1]]))['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward(nn.Module):\n",
    "    def __init__(self, gpt : GPT2LMHeadModel, question_mark_token, toxic_words : list, non_sense_response : list, eos_token = 0, device = \"cpu\", gpt_tokenizer = None, bos_token = 101) -> None:\n",
    "        super(Reward, self).__init__()\n",
    "        self.reward_coefficient = torch.ones(6, device = device) / 6\n",
    "        self.gpt = copy.deepcopy(gpt)\n",
    "        self.gpt = self.gpt.to(device)\n",
    "        self.gpt_tokenizer = BertTokenizer(vocab_file = os.path.join(pwd, 'GPT-2/vocab_small.txt')) if gpt_tokenizer is None else gpt_tokenizer\n",
    "        for p in self.gpt.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.device = device\n",
    "        self.eos_token = eos_token\n",
    "        self.bos_token = bos_token\n",
    "        self.question_mark_token = question_mark_token\n",
    "        self.bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "        self.bert = AutoModel.from_pretrained('ckiplab/bert-base-chinese').to(device)\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.toxic_words = toxic_words\n",
    "        self.non_sense_response = non_sense_response\n",
    "    def update_model(self, gpt):\n",
    "        self.gpt = copy.deepcopy(gpt)\n",
    "        for p in self.gpt.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.gpt.eval()\n",
    "    def to_device(self, device):\n",
    "        self.device = device\n",
    "    def update_reward_coefficient(self, gamma):\n",
    "        self.reward_coefficient =  copy.deepcopy(gamma)\n",
    "    def forward(self, state : list):\n",
    "        # state = [batch_size, {prev_utterance, response}] # elements of second dimension are dictionaries composed of previous utterance and subsequent response\n",
    "        # prev_utterance = [prev_utterance_seq_length, ]\n",
    "        # response = [response_seq_length, ]\n",
    "        self.gpt.eval()\n",
    "        for idx in range(len(state)):\n",
    "            state[idx]['prev_utterance'] = state[idx]['prev_utterance'].to(self.device)\n",
    "            state[idx]['response'] = state[idx]['response'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            reward = self.reward_coefficient[0] * (self.get_length_reward(state).exp()) # exp for avoiding +- offset each other, if you don't want it, go head delete .exp()\n",
    "            reward += self.reward_coefficient[1] * (self.get_question_reward(state).exp())\n",
    "            reward += self.reward_coefficient[2] * (self.get_coherence(state).exp())\n",
    "            reward += self.reward_coefficient[3] * (self.get_toxicity(state))\n",
    "            reward += self.reward_coefficient[4] * (self.get_ease_of_answering(state).exp())\n",
    "            reward += self.reward_coefficient[5] * (self.get_reward_semantic_coherence(state).exp())\n",
    "\n",
    "        return F.normalize(reward, dim = 0)\n",
    "    def get_response_prob(self, state, require_grad = False, reverse = False): # reverse means prev_utterance = response, response = prev_utterance\n",
    "        state = copy.deepcopy(state)\n",
    "        if reverse:\n",
    "            state = [{\"prev_utterance\" : state[idx]['response'], 'response' : state[idx]['prev_utterance']} for idx in range(len(state))]\n",
    "        def get_prob():\n",
    "            probability = torch.ones((len(state)), device = self.device)\n",
    "            for index, state_dict in enumerate(state):\n",
    "                utterance, response = state_dict['prev_utterance'].clone().detach(), state_dict['response'].clone().detach()\n",
    "                probability[index] *= self.p_seq2seq(utterance, response)\n",
    "            return F.normalize(probability, dim = 0)\n",
    "        self.gpt.train(require_grad)\n",
    "        if not require_grad:\n",
    "            with torch.no_grad():\n",
    "                return get_prob()\n",
    "        else:\n",
    "            return get_prob()\n",
    "    def p_seq2seq(self, up, down):\n",
    "        input_ids = up.clone().detach()\n",
    "        probability = 1e20 # refrain from continuously multiplying number 0 < number < 1 such that probability would be too small, which in turn would cause precision problem\n",
    "        for i in range(len(down)):\n",
    "            logits = self.gpt(input_ids = input_ids)['logits'] \n",
    "            logits = F.softmax(logits, dim = -1)\n",
    "            probability *= logits[-1, down[i]]\n",
    "            input_ids = torch.cat((input_ids, torch.tensor([down[i]], device = self.device)), dim = -1)\n",
    "            if down[i] == self.eos_token:\n",
    "                    break\n",
    "        return probability\n",
    "    def get_length_reward(self, state):\n",
    "        return F.normalize(torch.tensor([len(state[idx]['response']) for idx in range(len(state))], device = self.device, dtype = torch.float), dim = 0)\n",
    "    def get_question_reward(self, state):\n",
    "        return F.normalize(torch.tensor([1 if self.question_mark_token in state[idx]['response'] else 0 for idx in range(len(state))], device = self.device, dtype = torch.float), dim = 0)\n",
    "    def transform_from_gpt_to_bert_tokens(self, input_id):\n",
    "        original_string = self.gpt_tokenizer.decode(input_id, skip_special_tokens = True).replace(\" \", \"\")\n",
    "        return self.bert_tokenizer.encode(original_string, return_tensors = 'pt')[0].to(self.device)\n",
    "    def get_coherence(self, state):\n",
    "        state = [{\"prev_utterance\" : self.transform_from_gpt_to_bert_tokens(state[idx]['prev_utterance']), 'response' : self.transform_from_gpt_to_bert_tokens(state[idx]['response'])} for idx in range(len(state))]\n",
    "        coherence = torch.zeros((len(state)), device = self.device, dtype = torch.float)\n",
    "        cos = nn.CosineSimilarity(dim = -1)\n",
    "        for idx in range(len(state)):\n",
    "            utterance = self.bert(input_ids = state[idx]['prev_utterance'].unsqueeze(0))['last_hidden_state'][0][0]\n",
    "            response = self.bert(input_ids = torch.cat((torch.tensor([self.bos_token], device = self.device), state[idx]['response'])).unsqueeze(0))['last_hidden_state'][0][0]\n",
    "            sim = cos(utterance, response)\n",
    "            coherence[idx] = sim\n",
    "        return F.normalize(coherence, dim = 0)\n",
    "    def get_toxicity(self, state):\n",
    "        toxicity = []\n",
    "        for idx, value in enumerate(state):\n",
    "            counter = 0\n",
    "            for word in self.toxic_words:\n",
    "                if self.x_in_y(word, value['reponse']):\n",
    "                    counter -= 1\n",
    "            toxicity.append(counter)\n",
    "        return torch.tensor(toxicity, device = self.device)\n",
    "    def get_ease_of_answering(self, state):\n",
    "        ease_of_answering = torch.zeros((len(state)), device = self.device, dtype = torch.float)\n",
    "        for idx in range(len(state)):\n",
    "            temp = 0\n",
    "            for sentence in self.non_sense_response:\n",
    "                temp += self.p_seq2seq(state[idx]['response'], sentence) / len(sentence)\n",
    "            temp *= (- 1 / len(self.non_sense_response))\n",
    "            ease_of_answering[idx] = temp\n",
    "        return F.normalize(ease_of_answering, dim = 0)\n",
    "    def get_reward_semantic_coherence(self, state):\n",
    "        forward = self.get_response_prob(state)\n",
    "        backward = self.get_response_prob(state, reverse = True)\n",
    "        return F.normalize(torch.tensor([forward[idx] / len(state[idx]['response']) + backward[idx] / len(state[idx]['prev_utterance']) for idx in range(len(state))], device = self.device, dtype = torch.float), dim = 0)\n",
    "    def x_in_y(self, query, base):\n",
    "        try:\n",
    "            l = len(query)\n",
    "        except TypeError:\n",
    "            l = 1\n",
    "            query = type(base)((query,))\n",
    "\n",
    "        for i in range(len(base)):\n",
    "            if base[i : i+l] == query:\n",
    "                return True\n",
    "        return False\n",
    "    @staticmethod\n",
    "    def sentence2id(sentence, tokenizer, emotion_dict = {'其它': 0, '喜歡': 1, '悲傷': 2, '噁心': 3, '憤怒': 4, '開心': 5}, max_len = None, pad_to_max_len = False):\n",
    "\n",
    "        utterance = sentence[: sentence.index(']') + 1]\n",
    "        response = sentence[sentence.index(']') + 1:]\n",
    "        prev_utterance = tokenizer.encode(utterance, return_tensors = 'pt')[0]\n",
    "        response = tokenizer.encode(response, return_tensors = 'pt')[0][1:]\n",
    "        if max_len is not None and pad_to_max_len:\n",
    "            padding = max_len - prev_utterance.shape[-1]\n",
    "            if padding > 0:\n",
    "                prev_utterance = torch.cat((prev_utterance, torch.zeros(padding, dtype = torch.int64) - 1), dim = -1)\n",
    "            padding = max_len - response.shape[-1]\n",
    "            if padding > 0:\n",
    "                response = torch.cat((response, torch.zeros(padding, dtype = torch.int64) - 1), dim = -1)\n",
    "            \n",
    "        return {'prev_utterance' : prev_utterance, 'response' : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2DataSet(Dataset):\n",
    "    def __init__(self, tokenizer = None, max_len = 256, root_path = os.path.join(pwd, 'dataset'), train_path = 'single_emo_T_train.json', val_path = 'single_emo_T_valid.json', test_path = 'single_emo_T_test.json', status = 'train',\n",
    "                 shuffle = True) -> None:\n",
    "        self.file_path = os.path.join(root_path, (train_path if 'train' in status else (val_path if 'val' in status else test_path)))\n",
    "        with open(self.file_path) as f:\n",
    "            self.data = json.load(f)\n",
    "        self.dataset = []\n",
    "        self.tokenizer = BertTokenizer(vocab_file = os.path.join(pwd, 'GPT-2/vocab_small.txt')) if tokenizer is None else tokenizer\n",
    "        if  status == 'test':\n",
    "            self.data = [i[0] + i[1] for i in self.data]\n",
    "        self.max_len = max_len\n",
    "        for line in self.data:\n",
    "            if '[' not in line or ']' not in line or line.count(']') != 1 or line.count('[') != 1:\n",
    "                continue\n",
    "            else:\n",
    "                self.max_len = max(self.max_len, len(line))\n",
    "                self.dataset.append(line)\n",
    "        if shuffle:\n",
    "            random.shuffle(self.dataset)\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, index) -> dict:\n",
    "        return Reward.sentence2id(self.dataset[index], self.tokenizer, max_len = self.max_len)\n",
    "    @staticmethod\n",
    "    def get_toxic_ids_and_non_sense_response(tokenizer, \n",
    "                                         dirty_words = ['幹!','賤貨','米蟲','王八','王八蛋','不要臉','吃屎','敗類','智障','白癡','賤人','下流',\n",
    "                                                        '死肥豬','人渣','神經病','賤','尼瑪','無恥','婊','娘炮','魯蛇','廢物', '腦殘'],   \n",
    "                                         non_sense_sentences = ['嗯','嗯嗯','隨便啦','隨便啊','都可以','呵呵','哈哈','喔','笑死','是喔','好吧','我不知道',\n",
    "                                                                '還好','是啊','對啊','我也是','嘿嘿']):\n",
    "        # create toxic word list\n",
    "        toxic_ids = []\n",
    "        for i in range(len(dirty_words)):\n",
    "            ids = tokenizer.encode(dirty_words[i])\n",
    "            toxic_ids.append(ids[1:-1])\n",
    "        # create non sense sentence list\n",
    "        non_sense_ids = []\n",
    "        for i in range(len(non_sense_sentences)):\n",
    "            ids = tokenizer.encode(non_sense_sentences[i])\n",
    "            non_sense_ids.append(ids[1:])\n",
    "        return toxic_ids, non_sense_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GPT2DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DataLoader(g, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [25] at entry 0 and [42] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m a:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(a)\n\u001b[1;32m      3\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:128\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:128\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[1;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [25] at entry 0 and [42] at entry 1"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2769, 2695, 872, 102]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"我愛你\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, indices = F.softmax(gpt2(input_ids = torch.tensor([101, 2769, 2695, 872, 102]))['logits'], dim = -1).topk(3, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[9.9955e-01, 2.8439e-05, 1.7044e-05],\n",
       "         [1.3000e-01, 5.7022e-02, 4.0555e-02],\n",
       "         [3.6167e-01, 6.0442e-02, 5.2951e-02],\n",
       "         [1.8121e-01, 1.5723e-01, 9.7066e-02],\n",
       "         [3.8228e-01, 2.7874e-01, 8.7628e-02]], grad_fn=<TopkBackward0>),\n",
       " tensor([[ 103,  138, 8024],\n",
       "         [ 738,  947, 3221],\n",
       "         [ 872, 4638, 2769],\n",
       "         [ 138, 8024,  947],\n",
       "         [ 134, 5965, 5083]]))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[value[-1, i], indices[-1, i]] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor(0.3823, grad_fn=<SelectBackward0>), tensor(134)],\n",
       " [tensor(0.2787, grad_fn=<SelectBackward0>), tensor(5965)],\n",
       " [tensor(0.0876, grad_fn=<SelectBackward0>), tensor(5083)]]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 103, 1921, 4638, 3706,  679, 1962,  138, 1599, 3631,  140, 3221, 7433,\n",
       "        1921,  738, 3221, 2523, 5401, 4638,  101,  103, 2769,  738, 2682, 1391,\n",
       "        4125, 7102])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(input_ids = torch.tensor([101,  791, 1921, 1921, 3706, 4696, 1962,  138, 1599, 3631,  140,  102,\n",
    "         7433, 1921,  738, 3221, 2523, 5401, 4638,  101, 103, 2769, 738, 2682, 1391, 4125]))['logits'].argmax( dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(13317, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=13317, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:102 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "srf = gpt2.generate(torch.tensor([[101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,\n",
    " 102]]), max_length = 100, num_beams = 3, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 今 天 我 去 外 面 公 園 玩 [ 憤 怒 ] [SEP] 已 經 變 色 了 [CLS] [MASK] 你 是 最 棒 的 ， 摔 這 麼 美 的 ， 你 是 第 一 個 ！ [ 喜 歡 ] 你 是 最 棒 的 ， 摔 這 麼 美 的 ， 你 是 第 一 個 ！ [CLS] [MASK] 你 是 最 棒 的 ， 摔 這 麼 美 的 ， 你 是 第 一 個 ！ [ 喜 歡 ] 哈 哈 哈 哈 哈 哈 哈 哈 哈 哈 哈 哈 哈 哈'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(srf[0], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2,4,5,6][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 21128])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(input_ids = torch.tensor([[2,3,4]]))['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamHypotheses(object):\n",
    "    def __init__(self, num_beams, max_length = 200, length_penalty = 0.7):\n",
    "        self.max_length = max_length\n",
    "        self.length_penalty = length_penalty\n",
    "        self.num_beams = num_beams # beam size\n",
    "        self.beams = [] # best sequences and corresponding scores\n",
    "        self.worst_score = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.beams)\n",
    "\n",
    "    def add(self, hyp, mask, sum_logprobs, cur_len, utter_length, prev_utterance, response, eos_token):\n",
    "        score = sum_logprobs / (cur_len  ** self.length_penalty) # calculate penalized score\n",
    "        if len(self) < self.num_beams or score > self.worst_score:\n",
    "            if response[-1] != eos_token:\n",
    "                response = torch.cat((response, torch.tensor([eos_token])))\n",
    "            self.beams.append((score, hyp, mask, utter_length, sum_logprobs, prev_utterance, response))\n",
    "            if len(self) > self.num_beams:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _0, _1, _2, _3, _4, _5) in enumerate(self.beams)])\n",
    "                del self.beams[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score) if self.worst_score is not None else score\n",
    "\n",
    "    def is_done(self, best_sum_logprobs, cur_len):\n",
    "        if len(self) < self.num_beams:\n",
    "            return False\n",
    "        else:\n",
    "            cur_score = best_sum_logprobs / ((cur_len) ** self.length_penalty)\n",
    "            ret = self.worst_score >= cur_score\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "    def __init__(self, bert = None, bert_name = 'bert-base-chinese', bert_tokenizer = None, gpt_tokenizer = None, down_stream_features = 1024, only_down_stream = True, gamma = 0.9) -> None:\n",
    "        super(Q, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.bert_tokenizer = BertTokenizerFast.from_pretrained(bert_name) if bert_tokenizer is None else bert_tokenizer\n",
    "        self.bert = AutoModel.from_pretrained(bert_name) if bert is None else copy.deepcopy(bert)\n",
    "        self.gpt_tokenizer = BertTokenizer(vocab_file = os.path.join(pwd, 'GPT-2/vocab_small.txt')) if gpt_tokenizer is None else gpt_tokenizer\n",
    "        self.only_down_stream = only_down_stream\n",
    "        if only_down_stream:\n",
    "           self.bert.eval()\n",
    "           for p in self.bert.parameters():\n",
    "               p.require_grad = False\n",
    "        self.down_stream = nn.Sequential( nn.Linear(in_features = self.bert.pooler.dense.out_features, out_features = down_stream_features // 2),\n",
    "                                          nn.BatchNorm1d(down_stream_features // 2),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(in_features = down_stream_features // 2, out_features = down_stream_features),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(in_features = down_stream_features, out_features = down_stream_features),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Dropout(0.1),\n",
    "                                          nn.Linear(in_features = down_stream_features, out_features = down_stream_features // 4),\n",
    "                                          nn.BatchNorm1d(down_stream_features // 4),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(in_features = down_stream_features // 4, out_features = 1))\n",
    "    def transform_from_gpt_to_bert_tokens(self, input_id):\n",
    "        '''\n",
    "            input_ids : [batch_size, sequences]\n",
    "        '''\n",
    "        original_string = [self.gpt_tokenizer.decode(input, skip_special_tokens = True).replace(\" \", \"\") for input in input_id]\n",
    "        return [self.bert_tokenizer.encode(string, return_tensors = 'pt')[0] for string in original_string]\n",
    "    def get_processed(self, prev_utterance, bert_tokens = False, max_len = 256):\n",
    "            if not bert_tokens:\n",
    "                prev_utterance = self.transform_from_gpt_to_bert_tokens(prev_utterance)\n",
    "            input_ids = torch.zeros((len(prev_utterance), max_len)) - 1\n",
    "            for idx, utter in enumerate(prev_utterance):       \n",
    "                input_ids[idx][:min(max_len, len(utter))] = utter[:min(max_len, len(utter))]\n",
    "            mask = input_ids.ge(0)\n",
    "            input_ids[~mask] = 0\n",
    "            mask = mask.float()\n",
    "            return input_ids, mask\n",
    "    def forward(self, prev_utterance, response = None, mask = None, bert_tokens = False, max_len = 256, processed = False):\n",
    "        '''\n",
    "            prev_utterance : [batch_size, sequences]\n",
    "            response : [batch_size, sequences]\n",
    "            mask : [batch_size, sequences]\n",
    "        '''\n",
    "        prev_utterance = prev_utterance if response is None else [torch.cat((utt, res), dim = -1) for utt, res in zip(prev_utterance, response)]\n",
    "\n",
    "        # creating mask\n",
    "        if not processed or mask is None or not bert_tokens:\n",
    "            prev_utterance, mask = self.get_processed(prev_utterance = prev_utterance, bert_tokens = bert_tokens, max_len = max_len)\n",
    "        if self.only_down_stream:\n",
    "            with torch.no_grad():\n",
    "                up_stream = self.bert(input_ids = prev_utterance.to(torch.long), attention_mask = mask.to(torch.long))['last_hidden_state'][:, 0, :].view(len(prev_utterance), -1)\n",
    "        else:\n",
    "            up_stream = self.bert(input_ids = prev_utterance.to(torch.long), attention_mask = mask.to(torch.long))['last_hidden_state'][:, 0, :].view(len(prev_utterance), -1)\n",
    "        return self.down_stream(up_stream).view(len(prev_utterance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Wrapper(nn.Module):\n",
    "    def __init__(self, gpt = None, tokenizer = None, device = 'cpu'):\n",
    "        super(GPT2Wrapper, self).__init__()\n",
    "        self.gpt = gpt if gpt is not None else GPT2LMHeadModel.from_pretrained(os.path.join(pwd, './GPT-2/GPT2_finetune_1'))\n",
    "        self.tokenizer = BertTokenizer(vocab_file = os.path.join(pwd, 'GPT-2/vocab_small.txt')) if tokenizer is None else tokenizer\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "        self.special_tokens = {}\n",
    "        for idx, key in enumerate(self.tokenizer.all_special_tokens):\n",
    "            self.special_tokens[key] = self.tokenizer.all_special_ids[idx]\n",
    "        self.device = device\n",
    "    def forward(self, prev_utterance, response = None, beam = 3, max_len = 100, require_grad = True, device = 'mps'):\n",
    "        self.device = device\n",
    "        self.gpt.to(device)\n",
    "        if not require_grad:\n",
    "            with torch.no_grad():\n",
    "                    rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse = self.beam_search(prev_utterance, response = response, beam = beam, max_len = max_len)\n",
    "        else:\n",
    "                rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse = self.beam_search(prev_utterance, response = response, beam = beam, max_len = max_len)\n",
    "        return rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse\n",
    "    def get_prob(self, result, mask, prev_utterance, response):\n",
    "        utter_len = [len(utt) for utt in prev_utterance]\n",
    "        logits = torch.log(F.softmax(self.gpt(input_ids = result, attention_mask = mask)['logits'], dim = -1))\n",
    "        probs = torch.zeros(len(result))\n",
    "        for idx, res in enumerate(response):\n",
    "            res_len = res\n",
    "            for idx_, digit in enumerate(res):\n",
    "                probs[idx] += logits[idx][idx_ + utter_len[idx]][digit]\n",
    "            probs[idx] /= res_len\n",
    "        return probs\n",
    "    def beam_search(self, prev_utterance, response = None, eos_word = '[SEP]', beam = 3, max_len = 200, length_penalty_for_hypothesis = 0.8, emotion = ['喜歡', '悲傷', '噁心', '憤怒', '開心', '其它']):\n",
    "        '''\n",
    "            prev_utterance = [seq_length_prev_utterance]\n",
    "            response = [seq_length_response]\n",
    "            if response is None, mask and utter_length must not be None, which means they have been processed, i.e. use the output of this function to generate next response\n",
    "        '''\n",
    "        eos_token = self.special_tokens[eos_word]\n",
    "        # prev_utterance = prev_utterance if response is None else [torch.cat((utt, res)) for utt, res in zip(prev_utterance, response)]\n",
    "        batch_size = len(prev_utterance)\n",
    "        utter_length = torch.zeros((batch_size), dtype = torch.long)\n",
    "        ''' create mask '''\n",
    "        input_ids = torch.zeros((batch_size, max_len * 2), dtype = torch.long) - 1 # * 2 is for latter we will concatenate the generated sequences to them\n",
    "        with torch.no_grad():\n",
    "            for idx, utt in enumerate(prev_utterance):\n",
    "                    if response is not None: # means prev_utterance has not been processed\n",
    "                        if 138 not in response[idx][-5: ] and 140 not in response[idx][-5: ]:\n",
    "                            response[idx] = torch.cat((response[idx][:-1] if response[idx][-1] == eos_token else response[idx], self.tokenizer.encode('[' + emotion[random.randint(0, len(emotion) - 1)] + ']', return_tensors = 'pt')[0][1:]))\n",
    "                        elif response[idx][-1] != eos_token:\n",
    "                            response[idx] = torch.cat((response[idx], torch.tensor([eos_token])))    \n",
    "                        if 138 not in utt[-5: ] and 140 not in utt[-5: ]: # if emotion is not included in the sentence, randomly select a emotion type\n",
    "                            prev_utterance[idx] = torch.cat((utt[:-1] if utt[-1] == eos_token else utt, self.tokenizer.encode('[' + emotion[random.randint(0, len(emotion) - 1)] + ']', return_tensors = 'pt')[0][1:]))\n",
    "                        elif utt[-1] != eos_token:\n",
    "                            prev_utterance[idx] = torch.cat((prev_utterance[idx], torch.tensor([eos_token])))   \n",
    "                        prev_utterance[idx] = torch.cat((prev_utterance[idx], response[idx]))\n",
    "                        utter_length[idx] = min(len(prev_utterance[idx]), max_len)            \n",
    "                    else:\n",
    "                        length = len(utt[utt > 0])\n",
    "                        if 138 not in utt[length - 5:] and 140 not in utt[length - 5:]:\n",
    "                            start = length - 1 if utt[length - 1] == eos_token else length\n",
    "                            prev_utterance[idx][start: start + 5] = self.tokenizer.encode('[' + emotion[random.randint(0, len(emotion) - 1)] + ']', return_tensors = 'pt')[0][1:]\n",
    "                        elif utt[length - 1] != eos_token:\n",
    "                            prev_utterance[idx][length] = eos_token\n",
    "                        utter_length[idx] = len(prev_utterance[idx][prev_utterance[idx] > 0])\n",
    "                    input_ids[idx][: utter_length[idx]] = prev_utterance[idx][: utter_length[idx]]\n",
    "\n",
    "        mask = input_ids.ge(0)\n",
    "        input_ids[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        prev_utterance = input_ids\n",
    "        \n",
    "        beams_score = torch.zeros((batch_size, beam))\n",
    "        beams_score[:, 1:] = -1e9\n",
    "        beams_score = beams_score.view(-1) # [batch_size * beam]\n",
    "        beams_score = beams_score.to(self.device)\n",
    "    \n",
    "        utter_length = utter_length.unsqueeze(-1).expand((-1, beam)).contiguous().view(-1).to(self.device)\n",
    "        original_utter_length = utter_length.detach().clone()\n",
    "        \n",
    "        input_ids = prev_utterance.unsqueeze(1).expand((-1, beam, -1))\n",
    "        input_ids = input_ids.contiguous().view((batch_size * beam, max_len * 2)) # [batch_size * beam, max_len]\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        mask = mask.unsqueeze(1).expand((-1, beam, -1))\n",
    "        mask = mask.contiguous().view(-1, max_len * 2) # [batch_size * beam, max_len]\n",
    "        mask = mask.to(self.device)\n",
    "        '''\n",
    "            prev_utterance = [batch_size, max_len * 2]\n",
    "            input_ids = [batch_size * beam, max_len * 2]\n",
    "            mask = [batch_size * beam, max_len * 2] \n",
    "        '''\n",
    "\n",
    "        done = [False for _ in range(batch_size)]\n",
    "        hyps = [BeamHypotheses(num_beams = beam, max_length = max_len, length_penalty = length_penalty_for_hypothesis) for _ in range(self.vocab_size)]\n",
    "        for cur_len in range(max_len):\n",
    "            # print(input_ids[3], mask[3])\n",
    "            # global a, b\n",
    "            # if cur_len == max_len + 1:\n",
    "            #     a = input_ids\n",
    "            #     b = mask\n",
    "            #     return\n",
    "            out = torch.log(F.softmax(self.gpt(input_ids = input_ids, attention_mask = mask)['logits'].gather(index = (utter_length - 1).unsqueeze(-1).unsqueeze(-1).expand((-1, -1, self.vocab_size)), dim = 1), dim = -1))\n",
    "            # print(out.argmax(-1))\n",
    "            out = out.contiguous().view((batch_size, -1)) # [batch_size, beam * vocab_suze]\n",
    "            beams_score_next = beams_score.unsqueeze(-1).expand((-1, self.vocab_size)).contiguous().view(batch_size, -1) + out\n",
    "            next_scores, next_tokens = beams_score_next.topk(beam * 2, dim = -1)\n",
    "            # print(next_tokens)\n",
    "            next_beams = [] \n",
    "            for batch in range(batch_size):\n",
    "                next_beams_batch = []\n",
    "                batch_is_done = True\n",
    "                for score, token in zip(next_scores[batch], next_tokens[batch]):\n",
    "                    if len(next_beams_batch) >= beam:\n",
    "                        break\n",
    "                    beam_index = token // self.vocab_size\n",
    "                    real_token = token % self.vocab_size\n",
    "                    beam_index_for_input_ids = batch * beam + beam_index\n",
    "                    # print(score / ((cur_len) ** length_penalty_for_hypothesis), hyps[batch].worst_score, beam_index_for_input_ids)\n",
    "                    if (real_token == eos_token or cur_len == max_len - 1 or hyps[batch].worst_score is None or \\\n",
    "                    score / ((cur_len) ** length_penalty_for_hypothesis) > hyps[batch].worst_score or len(hyps[batch].beams) < beam) and cur_len >= 5:\n",
    "                        \n",
    "                        mask_beam = mask[beam_index_for_input_ids].detach().clone()\n",
    "                        input_id_beam = input_ids[beam_index_for_input_ids].detach().clone()\n",
    "                        mask_beam[utter_length[beam_index_for_input_ids]] = 1\n",
    "                        input_id_beam[utter_length[beam_index_for_input_ids]] = real_token\n",
    "\n",
    "                        hyps[batch].add(hyp = input_id_beam, mask = mask_beam, sum_logprobs = score, \n",
    "                                        cur_len = cur_len, utter_length = utter_length[beam_index_for_input_ids] + 1, \n",
    "                                        prev_utterance = input_id_beam[:original_utter_length[beam_index_for_input_ids]].detach().clone(),\n",
    "                                        response = input_id_beam[original_utter_length[beam_index_for_input_ids]: utter_length[beam_index_for_input_ids] + 1].detach().clone(),\n",
    "                                        eos_token = eos_token)\n",
    "                        if real_token != eos_token:\n",
    "                            next_beams_batch.append((score, real_token, beam_index_for_input_ids))\n",
    "                    else:\n",
    "                        next_beams_batch.append((score, real_token, beam_index_for_input_ids))\n",
    "                if batch_is_done:\n",
    "                    batch_is_done = hyps[batch].is_done(score, cur_len)\n",
    "                next_beams.extend(next_beams_batch)\n",
    "                if cur_len >= 20:\n",
    "                    done[batch] = batch_is_done if not done[batch] else True\n",
    "\n",
    "            beams_score = beams_score.new([x[0] for x in next_beams])\n",
    "            beam_token = input_ids.new([x[1] for x in next_beams])\n",
    "            beam_idx = input_ids.new([x[2] for x in next_beams])\n",
    "            utter_length = utter_length[beam_idx] + 1\n",
    "            original_utter_length = original_utter_length[beam_idx]\n",
    "            input_ids = input_ids[beam_idx, :]\n",
    "            mask = mask[beam_idx, :]\n",
    "            with torch.no_grad():\n",
    "                input_ids = input_ids.scatter(dim = 1, index = utter_length.unsqueeze(-1) - 1, src = beam_token.unsqueeze(-1).expand((-1, 2 * max_len)))\n",
    "                mask = mask.scatter(dim = 1, index = utter_length.unsqueeze(-1) - 1, src = torch.zeros((batch_size * beam, 2 * max_len)) + 1)\n",
    "            if all(done):\n",
    "                break\n",
    "        results = []\n",
    "        scores = []\n",
    "        masks = []\n",
    "        utter_len = []\n",
    "        sum_logprobs = []\n",
    "        result_prev_utterance = []\n",
    "        result_response = []\n",
    "        for batch in range(batch_size):\n",
    "            # (score, hyp, mask, utter_length, sum_logprobs)\n",
    "            results.append([])\n",
    "            scores.append([])\n",
    "            masks.append([])\n",
    "            utter_len.append([])\n",
    "            sum_logprobs.append([])\n",
    "            result_prev_utterance.append([])\n",
    "            result_response.append([])\n",
    "            for x in hyps[batch].beams: \n",
    "                result_prev_utterance[-1].append(x[5])\n",
    "                result_response[-1].append(x[6])     \n",
    "                sum_logprobs[-1].append(x[4])\n",
    "                utter_len[-1].append(x[3])\n",
    "                masks[-1].append(x[2])\n",
    "                results[-1].append(x[1])\n",
    "                scores[-1].append(x[0])\n",
    "            results[-1] = torch.stack(results[-1])\n",
    "            scores[-1] = torch.stack(scores[-1])\n",
    "            masks[-1] = torch.stack(masks[-1])\n",
    "            utter_len[-1] = torch.stack(utter_len[-1])\n",
    "            sum_logprobs[-1] = torch.stack(sum_logprobs[-1])\n",
    "        results = torch.stack(results)\n",
    "        masks = torch.stack(masks)\n",
    "        scores = torch.stack(scores)\n",
    "        utter_len = torch.stack(utter_len)\n",
    "        sum_logprobs = torch.stack(sum_logprobs)\n",
    "        return results, masks, scores, sum_logprobs, utter_len, result_prev_utterance, result_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 8, 9]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[10, 34,5,6,7,8,9][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch: int, gpt: GPT2Wrapper, Q_A: Q, Q_B: Q,  optimizer: torch.optim.Optimizer = None, R: Reward = None,\n",
    "                    dataset: Iterable = GPT2DataSet(), device: torch.device = 'cpu', batch_size = 4, max_len = 256, beam = 3, update_time_per_episode = 10, criterion = nn.MSELoss(), kl_control = 0.01,\n",
    "                    gpt_loss_coefficient = 0.1):\n",
    "    gpt.train()\n",
    "    # two Qs for Double DQN\n",
    "    Q_A.train()\n",
    "    Q_B.train()\n",
    "    kl_losses = []\n",
    "    gpt_losses = []\n",
    "    q_losses = []\n",
    "    total_losses = []\n",
    "    \n",
    "\n",
    "    with tqdm(total = len(dataset) // batch_size) as t:\n",
    "        for step in range(len(dataset) // batch_size):\n",
    "            t.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "            prev_utterance = []\n",
    "            response = []\n",
    "            for mini_step in range(step * batch_size, (step + 1) * batch_size):\n",
    "                pair = dataset[mini_step]\n",
    "                prev_utterance.append(pair['prev_utterance'])\n",
    "                response.append(pair['prev_utterance'])\n",
    "            utter_length = None\n",
    "            generate_time = 0\n",
    "            with torch.no_grad(): # generate episode\n",
    "                while (utter_length is None or all(utter_length < max_len)) and generate_time < 5:\n",
    "                    generate_time += 1\n",
    "                    if utter_length is None:\n",
    "                        rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse = gpt(prev_utterance = prev_utterance, response = response, beam = beam, max_len = max_len,\n",
    "                                                                device = device)\n",
    "                        # soft sampling\n",
    "                        previous_Q_distribution= F.softmax((Q_A(prev_utterance = rslt.view(batch_size * beam, -1), response = None, mask = None, bert_tokens = False, max_len = max_len * 2, processed = False) + \n",
    "                        Q_B(prev_utterance = rslt.view(batch_size * beam, -1), response = None, mask = None, bert_tokens = False, max_len = max_len * 2, processed = False)).view(batch_size, -1), dim = -1)\n",
    "                        select = torch.multinomial(previous_Q_distribution, 1)\n",
    "                        previous_result = rslt.gather(index = select.unsqueeze(-1).expand(-1, -1, max_len * 2), dim = 1).squeeze(1)\n",
    "                        results = previous_result.detach().clone()\n",
    "                        masks = msk.gather(index = select.unsqueeze(-1).expand(-1, -1, max_len * 2), dim = 1).squeeze(1)\n",
    "                        scores = sco.gather(index = select, dim = 1).squeeze(1)\n",
    "                        utter_length = utrlen.gather(index = select, dim = 1).squeeze(1)\n",
    "                        sum_logprobs = smlgprbs.gather(index = select, dim = 1).squeeze(1)\n",
    "                        results_prev_utterance = [rsltprvrnce[i][int(select[i])] for i in range(2)]\n",
    "                        results_response = [rsltprvrnce[i][int(select[i])] for i in range(2)]\n",
    "                    else:\n",
    "                        rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse = gpt(prev_utterance = previous_result, beam = beam, max_len = max_len,\n",
    "                                                            device = device)\n",
    "\n",
    "                        previous_Q_distribution = F.softmax((Q_A(prev_utterance = rslt.view(batch_size * beam, -1), response = None, mask = None, bert_tokens = False, max_len = max_len * 2, processed = False) + \n",
    "                        Q_B(prev_utterance = rslt.view(batch_size * beam, -1), response = None, mask = None, bert_tokens = False, max_len = max_len * 2, processed = False)).view(batch_size, -1), dim = -1)\n",
    "                        select = torch.multinomial(previous_Q_distribution, 1)\n",
    "                        previous_result = rslt.gather(index = select.unsqueeze(-1).expand(-1, -1, max_len * 2), dim = 1).squeeze(1)\n",
    "                        \n",
    "                        results = torch.cat((results, previous_result.detach().clone()), dim = 0)\n",
    "                        masks = torch.cat((masks, msk.gather(index = select.unsqueeze(-1).expand(-1, -1, max_len * 2), dim = 1).squeeze(1)), dim = 0)\n",
    "                        scores = torch.cat((scores, sco.gather(index = select, dim = 1).squeeze(1)), dim = 0)\n",
    "                        utter_length = torch.cat((utter_length, utrlen.gather(index = select, dim = 1).squeeze(1)), dim = 0)\n",
    "                        sum_logprobs = torch.cat((sum_logprobs, smlgprbs.gather(index = select, dim = 1).squeeze(1)), dim = 0)\n",
    "                        results_prev_utterance.extend([rsltprvrnce[i][int(select[i])] for i in range(2)])\n",
    "                        results_response.extend([rsltprvrnce[i][int(select[i])] for i in range(2)])\n",
    "                reward = R([{'prev_utterance': utt, 'response': res} for utt, res in zip(results_prev_utterance, results_response)])\n",
    "                results_Q, mask_Q = Q_A.get_processed(prev_utterance = results, bert_tokens = False, max_len = max_len * 2)\n",
    "            for update_time in range(update_time_per_episode):\n",
    "                if random.random() >= 0.5: # update Q_A\n",
    "                    q_estimate = Q_A.forward(prev_utterance = results_Q[: len(results_Q) - batch_size], response = None, mask = mask_Q[: len(results_Q) - batch_size], bert_tokens = True, max_len = max_len * 2, processed = True)\n",
    "                    q_target = reward + Q_A.gamma * Q_B.forward(prev_utterance = results_Q[batch_size: ], response = None, mask = mask_Q[batch_size: ], bert_tokens = True, max_len = max_len * 2, processed = True)\n",
    "                else: # update Q_B\n",
    "                    q_estimate = Q_B.forward(prev_utterance = results_Q[: len(results_Q) - batch_size], response = None, mask = mask_Q[: len(results_Q) - batch_size], bert_tokens = True, max_len = max_len * 2, processed = True)\n",
    "                    q_target = reward + Q_B.gamma * Q_A.forward(prev_utterance = results_Q[batch_size: ], response = None, mask = mask_Q[batch_size: ], bert_tokens = True, max_len = max_len * 2, processed = True)\n",
    "                q_loss = criterion(q_estimate, q_target) \n",
    "                kl_loss = - kl_control * torch.log(torch.mean(q_estimate.exp()))\n",
    "                probs = gpt.get_prob(results[: len(results_Q) - batch_size], masks[: len(results_Q) - batch_size], results_prev_utterance[: len(results_Q) - batch_size], results_response[: len(results_Q) - batch_size])\n",
    "                gpt_loss = torch.sum(probs.detach().clone().exp() * probs * q_target)\n",
    "                total_loss = q_loss + kl_control * kl_loss + gpt_loss_coefficient * gpt_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                kl_losses.append(kl_loss)\n",
    "                gpt_losses.append(gpt_loss)\n",
    "                q_losses.append(q_loss)\n",
    "                total_losses.append(total_loss)\n",
    "                t.set_postfix(klloss = np.mean(kl_losses[-1]), gpt_loss = np.mean(gpt_losses[-1]),\n",
    "                          q_loss = np.mean(q_losses[-1]), total_loss = np.mean(total_losses[-1]))\n",
    "            t.update(1)\n",
    "    return np.mean(q_losses), np.mean(kl_losses), np.mean(gpt_losses), np.mean(total_losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_dir(pwd):\n",
    "    runs_dir = os.path.join(pwd, 'runs')\n",
    "    train_dir = os.path.join(runs_dir, 'train')\n",
    "    if not os.path.exists(runs_dir):\n",
    "        os.mkdir(runs_dir)\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.mkdir(train_dir)\n",
    "    counter = 1\n",
    "    exp = f\"exp{counter}\"\n",
    "    while exp in os.listdir(train_dir):\n",
    "        counter += 1\n",
    "        exp = f\"exp{counter}\"\n",
    "    output_dir = os.path.join(train_dir, exp)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    if not os.path.exists(os.path.join(output_dir, 'models')):\n",
    "        os.mkdir(os.path.join(output_dir, 'models'))\n",
    "    with open(os.path.join(output_dir, 'log.csv'), 'w', newline = '') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Epoch', 'KLLoss', 'GPTLoss', 'QLoss', 'TOTALLoss'])\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102068736"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in gpt2.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2402, 106],\n",
       " [6547, 6515],\n",
       " [5101, 6100],\n",
       " [4374, 1061],\n",
       " [4374, 1061, 6028],\n",
       " [679, 6206, 5622],\n",
       " [1391, 2241],\n",
       " [3134, 7546],\n",
       " [3255, 7397],\n",
       " [4635, 4622],\n",
       " [6547, 782],\n",
       " [678, 3837],\n",
       " [3647, 5503, 6500],\n",
       " [782, 3942],\n",
       " [4868, 5195, 4567],\n",
       " [6547],\n",
       " [2225, 4454],\n",
       " [4192, 2615],\n",
       " [2040],\n",
       " [2023, 4152],\n",
       " [7798, 6026],\n",
       " [2450, 4289],\n",
       " [5582, 3659]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_words, non_sense_response = GPT2DataSet.get_toxic_ids_and_non_sense_response(gpt2_tokenizer)\n",
    "toxic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:  311069698\n",
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1705075 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "weight_decay = 1e-4\n",
    "epochs = 16\n",
    "lr_drop = 8\n",
    "Q_discounter_factor = 0.9\n",
    "kl_control = 0.01\n",
    "gpt_loss_coefficient = 0.1\n",
    "pwd = os.getcwd()\n",
    "data_root = os.path.join(pwd, 'dataset')\n",
    "assert os.path.exists(data_root)\n",
    "output_dir = get_output_dir(pwd)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(os.path.join(pwd, 'GPT-2/GPT2_finetune_2'))\n",
    "bert = BertModel.from_pretrained('bert-base-chinese')\n",
    "gpt2_tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "gpt_wrapper = GPT2Wrapper(gpt2, tokenizer = gpt2_tokenizer, device = device)\n",
    "Q_A = Q(gpt_tokenizer = gpt2_tokenizer, gamma = Q_discounter_factor, bert_name = 'bert-base-chinese')\n",
    "Q_B = Q(gpt_tokenizer = gpt2_tokenizer, gamma = Q_discounter_factor, bert_name = 'bert-base-chinese')\n",
    "toxic_words, non_sense_response = GPT2DataSet.get_toxic_ids_and_non_sense_response(gpt2_tokenizer)\n",
    "R = Reward(gpt = gpt2, question_mark_token = 136, toxic_words = toxic_words, gpt_tokenizer = gpt2_tokenizer,\n",
    "               non_sense_response = non_sense_response, eos_token = 102, device = device, bos_token = 101)\n",
    "criterion = nn.MSELoss()\n",
    "gpt_wrapper.to(device)\n",
    "Q_A.to(device)\n",
    "Q_B.to(device)\n",
    "optimizer = torch.optim.Adam([{ 'params': [p for p in gpt_wrapper.parameters() if p.requires_grad]},\n",
    "                              { 'params': [p for p in Q_A.parameters() if p.requires_grad]},\n",
    "                              { 'params': [p for p in Q_B.parameters() if p.requires_grad]},\n",
    "                              ], lr = lr, weight_decay = weight_decay)\n",
    "print('total parameters: ', sum([p.numel() for p in gpt_wrapper.parameters() if p.requires_grad]) + \n",
    "      sum([p.numel() for p in Q_A.parameters() if p.requires_grad]) + \n",
    "      sum([p.numel() for p in Q_B.parameters() if p.requires_grad]))\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_drop)\n",
    "train_dataset = GPT2DataSet(tokenizer = gpt2_tokenizer, max_len = 256, root_path = data_root, status = 'train')\n",
    "test_dataset = GPT2DataSet(tokenizer = gpt2_tokenizer, max_len = 256, root_path = data_root, status = 'test')\n",
    "q_losses = []\n",
    "gpt_losses = []\n",
    "kl_losses = []\n",
    "total_losses = []\n",
    "print('Start training')\n",
    "for epoch in range(epochs):\n",
    "    q_loss, kl_loss, gpt_loss, total_loss = train_one_epoch(epoch = epoch, gpt = gpt_wrapper, Q_A = Q_A, Q_B = Q_B,\n",
    "                                                            optimizer = optimizer, R = R, dataset = train_dataset, device = device, batch_size = batch_size,\n",
    "                                                            max_len = 256, beam = 3, update_time_per_episode = 10, criterion = criterion, \n",
    "                                                            kl_control = kl_control, gpt_loss_coefficient = gpt_loss_coefficient)\n",
    "    q_losses.append(q_loss)\n",
    "    kl_losses.append(kl_loss)\n",
    "    gpt_losses.append(gpt_loss)\n",
    "    total_losses.append(total_losses)\n",
    "    torch.save({\n",
    "            'Q_A': Q_A.state_dict(),\n",
    "            'Q_B': Q_B.state_dict(),\n",
    "            'GPT': gpt_wrapper.state_dict()\n",
    "        }, os.path.join(output_dir, 'models/latest.pth'))\n",
    "    if min(q_losses) == q_loss:\n",
    "        torch.save({\n",
    "            'Q_A': Q_A.state_dict(),\n",
    "            'Q_B': Q_B.state_dict(),\n",
    "            'GPT': gpt_wrapper.state_dict()\n",
    "        }, os.path.join(output_dir, 'models/best_q_loss.pth'))\n",
    "    if min(kl_losses) == kl_loss:\n",
    "        torch.save({\n",
    "            'Q_A': Q_A.state_dict(),\n",
    "            'Q_B': Q_B.state_dict(),\n",
    "            'GPT': gpt_wrapper.state_dict()\n",
    "        }, os.path.join(output_dir, 'models/best_kl_loss.pth'))\n",
    "    if min(gpt_losses) == gpt_loss:\n",
    "        torch.save({\n",
    "            'Q_A': Q_A.state_dict(),\n",
    "            'Q_B': Q_B.state_dict(),\n",
    "            'GPT': gpt_wrapper.state_dict()\n",
    "        }, os.path.join(output_dir, 'models/best_gpt_loss.pth'))\n",
    "    if min(total_losses) == total_loss:\n",
    "        torch.save({\n",
    "            'Q_A': Q_A.state_dict(),\n",
    "            'Q_B': Q_B.state_dict(),\n",
    "            'GPT': gpt_wrapper.state_dict()\n",
    "        }, os.path.join(output_dir, 'models/best_total_loss.pth'))\n",
    "    with open(os.path.join(output_dir, 'log.csv'), 'a', newline = '') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([epoch, kl_loss, gpt_loss, q_loss, total_loss])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(pwd, 'runs/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./runs/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "aq = Q()\n",
    "bq = Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(353404)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(rslt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "tensor([22, 22, 22, 26, 26, 26, 32, 32, 32, 34, 34, 34])\n",
      "3\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n",
      "tensor([34, 34, 34, 50, 50, 50, 57, 57, 57, 49, 49, 49])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_one_epoch(gpt \u001b[39m=\u001b[39;49m g, Q_A \u001b[39m=\u001b[39;49m aq, Q_B \u001b[39m=\u001b[39;49m bq)\n",
      "Cell \u001b[0;32mIn [20], line 34\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(gpt, Q_A, Q_B, optimizer, dataset, device, batch_size, max_len, beam)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse \u001b[39m=\u001b[39m gpt(prev_utterance \u001b[39m=\u001b[39;49m previous_result, beam \u001b[39m=\u001b[39;49m beam, max_len \u001b[39m=\u001b[39;49m max_len,\n\u001b[1;32m     35\u001b[0m                                         device \u001b[39m=\u001b[39;49m device)\n\u001b[1;32m     37\u001b[0m     previous_Q_distribution \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax((Q_A(prev_utterance \u001b[39m=\u001b[39m rslt\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m beam, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), response \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, bert_tokens \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, max_len \u001b[39m=\u001b[39m max_len \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, processed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m+\u001b[39m \n\u001b[1;32m     38\u001b[0m     Q_B(prev_utterance \u001b[39m=\u001b[39m rslt\u001b[39m.\u001b[39mview(batch_size \u001b[39m*\u001b[39m beam, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), response \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, bert_tokens \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, max_len \u001b[39m=\u001b[39m max_len \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, processed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m))\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     select \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(previous_Q_distribution, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [8], line 18\u001b[0m, in \u001b[0;36mGPT2Wrapper.forward\u001b[0;34m(self, prev_utterance, response, beam, max_len, require_grad, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m             rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(prev_utterance, response \u001b[39m=\u001b[39m response, beam \u001b[39m=\u001b[39m beam, max_len \u001b[39m=\u001b[39m max_len)\n\u001b[1;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(prev_utterance, response \u001b[39m=\u001b[39;49m response, beam \u001b[39m=\u001b[39;49m beam, max_len \u001b[39m=\u001b[39;49m max_len)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse\n",
      "Cell \u001b[0;32mIn [8], line 89\u001b[0m, in \u001b[0;36mGPT2Wrapper.beam_search\u001b[0;34m(self, prev_utterance, response, eos_word, beam, max_len, length_penalty_for_hypothesis, emotion)\u001b[0m\n\u001b[1;32m     81\u001b[0m hyps \u001b[39m=\u001b[39m [BeamHypotheses(num_beams \u001b[39m=\u001b[39m beam, max_length \u001b[39m=\u001b[39m max_len, length_penalty \u001b[39m=\u001b[39m length_penalty_for_hypothesis) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size)]\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m cur_len \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_len):\n\u001b[1;32m     83\u001b[0m     \u001b[39m# print(input_ids[3], mask[3])\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39m# global a, b\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39m#     b = mask\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[39m#     return\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt(input_ids \u001b[39m=\u001b[39;49m input_ids, attention_mask \u001b[39m=\u001b[39;49m mask)[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mgather(index \u001b[39m=\u001b[39m (utter_length \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size)), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m), dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     90\u001b[0m     \u001b[39m# print(out.argmax(-1))\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview((batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m# [batch_size, beam * vocab_suze]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1046\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1046\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1047\u001b[0m     input_ids,\n\u001b[1;32m   1048\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1049\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1050\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1051\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1052\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1053\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1054\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1055\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1056\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1057\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1058\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1059\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1060\u001b[0m )\n\u001b[1;32m   1061\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:889\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    879\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    880\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    881\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    890\u001b[0m         hidden_states,\n\u001b[1;32m    891\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    892\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    893\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    894\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    895\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    896\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    897\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    900\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    387\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 389\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    390\u001b[0m     hidden_states,\n\u001b[1;32m    391\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    392\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    393\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    394\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    395\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    397\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:330\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    328\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    332\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:211\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[1;32m    210\u001b[0m attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mtype(value\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 211\u001b[0m attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_dropout(attn_weights)\n\u001b[1;32m    213\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_one_epoch(gpt = g, Q_A = aq, Q_B = bq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2371, 2218,  671,  943, 2099,  138, 1599, 3631,  140,  102,  101,\n",
       "        2371, 2218,  671,  943, 2099,  138, 1599, 3631,  140,  102, 4638, 6929,\n",
       "        1372, 3291, 3472, 8013,  101,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GPT2Wrapper(gpt = gpt2, tokenizer = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "Qa = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3341],\n",
       "        [-0.6686],\n",
       "        [-0.2113],\n",
       "        [-0.6384],\n",
       "        [-0.0766],\n",
       "        [-0.3934]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qa(prev_utterance = rslt.view(2 * 3, -1), response = None, mask = None, bert_tokens = False, max_len = 100 * 2, processed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = F.softmax(Qa(prev_utterance = rslt.view(2 * 3, -1), response = None, mask = None, bert_tokens = False, max_len = 100 * 2, processed = False).view(2, -1), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [1]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = torch.multinomial(dis, 1)\n",
    "select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7753, -0.7489, -0.7135],\n",
       "         [-0.6005, -0.6241, -0.6006]], grad_fn=<StackBackward0>),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sco, smlgprbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sco.gather(index = select, dim = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n",
      "tensor([15, 15, 15,  9,  9,  9])\n"
     ]
    }
   ],
   "source": [
    "rslt, msk, sco, smlgprbs, utrlen, rsltprvrnce, rsltrspse = g([torch.tensor([101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,\n",
    " 102]), torch.tensor([101, 2769, 2695, 872,  138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140, 102])], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102]),\n",
       "   tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102]),\n",
       "   tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102])],\n",
       "  [tensor([ 101, 2769, 2695,  872,  138, 2734, 2584,  140,  102]),\n",
       "   tensor([ 101, 2769, 2695,  872,  138, 2734, 2584,  140,  102]),\n",
       "   tensor([ 101, 2769, 2695,  872,  138, 2734, 2584,  140,  102])]],\n",
       " [[tensor([2111, 2094,  679, 4761, 6887,  872,  947, 3221, 6306, 1485, 8043, 8043,\n",
       "           8043,  101,  102]),\n",
       "   tensor([3221, 4158,  749, 1343, 1912, 7481, 7087, 7509, 1621, 8043, 8043, 8043,\n",
       "            101,  103,  102]),\n",
       "   tensor([2111, 2094,  679, 4761, 6887,  872,  947, 3221, 6306, 1485, 8043, 8043,\n",
       "           8043,  101,  103,  102])],\n",
       "  [tensor([ 872, 4638, 6929,  943, 3582, 3298,  102]),\n",
       "   tensor([2769, 1652,  511, 1464,  872, 2695, 4294, 2769, 3051,  102]),\n",
       "   tensor([2769, 1652,  511, 1464,  872, 2695, 4294, 2769, 3051, 2241,  102])]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsltprvrnce, rsltrspse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "         2584,  140,  102, 2111, 2094,  679, 4761, 6887,  872,  947, 3221, 6306,\n",
       "         1485, 8043, 8043, 8043,  101,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "         2584,  140,  102, 3221, 4158,  749, 1343, 1912, 7481, 7087, 7509, 1621,\n",
       "         8043, 8043, 8043,  101,  103,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0],\n",
       "        [ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "         2584,  140,  102, 2111, 2094,  679, 4761, 6887,  872,  947, 3221, 6306,\n",
       "         1485, 8043, 8043, 8043,  101,  103,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([8043,  792, 3221, 4158, 1567, 8043,  102]),\n",
       "  tensor([8043,  792, 3221, 4158,  784, 7938, 8043, 6538,  982, 3043, 3043, 1765,\n",
       "          1553, 7619,  746,  100,  101,  102]),\n",
       "  tensor([8043,  792, 3221, 4158,  784, 7938, 8043, 6538,  982, 3043, 3043, 1765,\n",
       "          1553, 7619,  746,  100,  101,  103,  102])],\n",
       " [tensor([3221, 2769, 2061, 2874, 7097,  749,  102]),\n",
       "  tensor([3221, 2769, 2061, 2874,  749, 2769, 4638, 2894, 1462, 8024, 2769, 2894,\n",
       "          1462, 4638, 2894, 1462, 4638, 2894, 1462, 4638,  102]),\n",
       "  tensor([3221, 2769, 2061, 2874,  749, 2769, 4638, 2894, 1462, 8024, 2769, 2894,\n",
       "          1462, 4638, 2894, 1462, 4638, 2894, 1462, 4638, 2894,  102])]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsltrspse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([8043,  792, 3221, 4158,  784, 7938, 8043, 6538,  982, 3043, 3043, 1765,\n",
       "         1553, 7619,  746,  100,  101,  103,  102]),\n",
       " tensor([3221, 2769, 2061, 2874,  749, 2769, 4638, 2894, 1462, 8024, 2769, 2894,\n",
       "         1462, 4638, 2894, 1462, 4638, 2894, 1462, 4638,  102])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rsltrspse[index_selected][int(select[index_selected])] for index_selected in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt, msk, sco, smlgprbs, utrlen = g(rslt[0], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 今 天 我 去 外 面 公 園 玩 [ 憤 怒 ] [SEP] ？ 介 是 為 啥 ？ [ 憤 怒 ] [SEP] 騙 人 的 嗎 ？ [CLS] [MASK] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0].shape\n",
    "r([[-2.8096, -6.8823, -6.8823],\n",
    "        [-2.1762, -6.5803, -6.5984]], gra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9054],\n",
       "         [ 2.1621],\n",
       "         [-0.3211]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = h.gather(index = torch.tensor([[2], [1], [0]]), dim = 1)\n",
    "g, g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m h\u001b[39m.\u001b[39mscatter(dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m2\u001b[39m], [\u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m]]), src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros())\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of SymInts size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "h.scatter(dim = 1, index = torch.tensor([[2], [1], [0]]), src = torch.zeros())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[13.6946, 13.5733, 11.8984, 11.7583, 11.6346, 11.5276]],\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[ 947, 5582, 8043, 1557,  134, 4638]]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(input_ids = a[3].unsqueeze(0), attention_mask = b[3].unsqueeze(0).to(torch.long))['logits'][:, 9, :].topk(6, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[[19.3966, 16.6512, 15.6357, 14.6237, 14.0722, 13.7992],\n",
       "         [17.7092, 17.1337, 17.0325, 16.8664, 16.7009, 16.6512],\n",
       "         [18.5763, 15.8345, 15.3630, 15.3226, 14.5392, 13.3719],\n",
       "         [18.1575, 17.4653, 15.9241, 14.5630, 14.4235, 14.4225],\n",
       "         [15.6550, 15.1796, 15.0827, 13.2894, 12.9438, 11.5672],\n",
       "         [14.6270, 12.9387, 12.3596, 12.2733, 11.1103, 10.4297],\n",
       "         [19.9478, 15.4028, 13.7760, 13.4685, 13.0539, 12.9692],\n",
       "         [16.9682, 13.9107, 13.6344, 13.5101, 12.8403, 11.6358],\n",
       "         [17.5265, 12.5370, 12.4661, 11.6980, 11.4350, 11.1611],\n",
       "         [14.6954, 11.4877, 11.4264, 11.2573, 11.2200, 10.9985]]],\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[[2241, 5175, 4649, 3221,  872, 1889],\n",
       "         [1995, 6552, 2094, 2336, 5529, 6303],\n",
       "         [ 872, 2094, 1162, 8043, 8013, 3221],\n",
       "         [5175, 2094, 4863, 3506, 2241,  712],\n",
       "         [3221, 8013,  511, 8024,  100, 4638],\n",
       "         [1567,  943, 2769, 8043,  784, 6306],\n",
       "         [ 100, 8013,  121,  511, 2769,  939],\n",
       "         [1762, 7279, 8043,  939, 8024,  889],\n",
       "         [1814, 1468, 2582,  889, 4638, 3333],\n",
       "         [2769, 2208, 7531, 2471, 3857,  140]]]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(input_ids = torch.tensor([[ 101, 2769, 2695,  872,  138, 2734, 2584,  140,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,  102, 3221,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0]]), attention_mask = torch.tensor([[1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "         0., 0.]]))['logits'][:, 95:105, :].topk(6, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[19.6527, 13.3922, 12.5040, 12.1969, 10.9196,  9.9409]],\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[2061, 7464, 6873, 4638, 3553,  982]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2(input_ids = torch.tensor([[101, 2769, 2695, 872,  138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140, 102, 3221, 2769]]))['logits'][:, -1, :].topk(6, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('[CLS] 今 天 我 去 外 面 公 園 玩 [ 憤 怒 ] [SEP] ？ 介 是 為 啥',\n",
       " tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "         2584,  140,  102, 8043,  792, 3221, 4158, 1567]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gpt2.generate(input_ids = torch.tensor([[101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,\n",
    " 102]]), num_beams = 3)[0]), gpt2.generate(input_ids = torch.tensor([[101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,\n",
    " 102]]), num_beams = 3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(511)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "           0,    0,    0,    0,    0,    0,    0,    0])\n",
    "gpt2(input_ids = input_ids, attention_mask = (input_ids > 0).float())['logits'].argmax(-1)[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 5], [1, 2, 3], [1, 2, 5]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "t.extend([[1,2,3], [1,2,5]])\n",
    "t.extend([[1,2,3], [1,2,5]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [1, 2, 5]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, indices = g.topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((g, torch.zeros((5, 1)) + 1), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/q61hgngn7zjfnxbjqlzc_v9m0000gn/T/ipykernel_66931/3772463029.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  value, indices = F.softmax(gpt2(input_ids = prev_utterance)['logits']).topk(beam, dim = -1)\n"
     ]
    }
   ],
   "source": [
    "result, sr, score = gg(torch.tensor([[101,\n",
    " 791,\n",
    " 1921,\n",
    " 2769,\n",
    " 1343,\n",
    " 1912,\n",
    " 7481,\n",
    " 1062,\n",
    " 1754,\n",
    " 4381,\n",
    " 138,\n",
    " 2734,\n",
    " 2584,\n",
    " 140,\n",
    " 102]]), max_len = 100, beam = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[tensor(-5.5424, grad_fn=<AddBackward0>),\n",
       "   tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102, 2347, 5195, 6365, 5682,  749,  101,  103])],\n",
       "  [tensor(-6.1180, grad_fn=<AddBackward0>),\n",
       "   tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102, 2347, 5195, 4255, 4634,  749,  101,  103])],\n",
       "  [tensor(-6.3388, grad_fn=<AddBackward0>),\n",
       "   tensor([ 101,  791, 1921, 2769, 1343, 1912, 7481, 1062, 1754, 4381,  138, 2734,\n",
       "           2584,  140,  102, 2347, 5195, 6365, 5622,  749,  101,  103])]]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.5424, -6.1180, -6.3388]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([2347, 5195, 6365, 5682,  749,  101,  103]),\n",
       "  tensor([2347, 5195, 4255, 4634,  749,  101,  103]),\n",
       "  tensor([2347, 5195, 6365, 5622,  749,  101,  103])]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已 經 變 色 了 我\n",
      "已 經 變 色 了 你\n",
      "已 經 爆 發 了 我\n",
      "已 經 爆 發 了 你\n",
      "已 經 變 臉 了 我\n"
     ]
    }
   ],
   "source": [
    "for i in sr[0]:\n",
    "    print(tokenizer.decode(i[1], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, indices = F.softmax(gpt2(input_ids = torch.tensor([101, 2769, 2695, 872, 102]))['logits'], dim = -1).topk(3, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7166e-05, -1.1302e+01, -1.3569e+01],\n",
       "         [-2.4632e+00, -2.5122e+00, -2.8395e+00],\n",
       "         [-4.5884e-01, -2.5241e+00, -3.6483e+00],\n",
       "         [-1.1748e+00, -2.0745e+00, -2.3433e+00],\n",
       "         [-1.3878e+00, -1.6446e+00, -2.4694e+00]], grad_fn=<LogBackward0>),\n",
       " tensor([[ 103,  140,  101],\n",
       "         [ 738, 4638,  947],\n",
       "         [ 872, 4638, 1391],\n",
       "         [ 138,  101, 8024],\n",
       "         [ 112,  138,  100]]),\n",
       " torch.Size([5, 3]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(value), indices, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19,  19, 112])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.tensor([19, 19]), indices[-1][0].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "q = Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = [torch.tensor([101, 2769, 2695, 872, 102]), torch.tensor([101, 2769, 2695, 872, 34, 234, 45, 102])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7675e-01],\n",
       "        [2.7341e-04]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(prev_utterance = prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.randn(10)\n",
    "h = torch.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = g.ge(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GPT2DataSet(shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_utterance': tensor([ 101, 4412, 1762, 6917, 6206, 4500, 3265, 3706, 8043,  138, 1599, 3631,\n",
       "          140,  102]),\n",
       " 'response': tensor([4412, 1762, 3760,  749,  102])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.2095e-01,  5.9866e-01, -1.1736e+00,  ...,  1.3144e+00,\n",
       "          -6.6073e-01,  3.1353e-01],\n",
       "         [-2.5177e-01,  5.4246e-01, -1.5848e-01,  ...,  1.7940e-01,\n",
       "          -5.1476e-01, -4.9664e-02],\n",
       "         [-3.1723e-01,  4.4264e-01, -1.4355e-01,  ...,  1.8843e-01,\n",
       "          -6.1138e-01,  1.0900e-03]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.9996,  0.9367,  0.9999,  0.9204,  0.8145, -0.3277, -0.8850, -0.9492,\n",
       "          0.9857, -0.9993,  1.0000,  1.0000, -0.8858, -0.7393,  0.9996, -0.9996,\n",
       "         -0.5293,  0.0339,  0.9951,  0.1820,  0.9798, -0.9997,  0.0852, -0.8749,\n",
       "         -0.5547,  0.9978,  0.7787, -0.9091, -0.9961,  0.9996,  0.9721,  0.9997,\n",
       "          0.8698, -0.9885, -0.9998,  0.5070,  0.5023,  0.9952,  0.8226, -0.8865,\n",
       "         -0.9540,  0.4088, -0.2360, -0.9980, -0.6263,  0.6001, -1.0000, -0.9999,\n",
       "         -0.9786,  0.9238, -0.7652, -1.0000,  0.9813, -0.9630, -0.8354,  0.9973,\n",
       "         -0.9973,  0.6421,  1.0000,  0.6468,  0.9996, -0.9192,  0.8512, -0.9993,\n",
       "          0.9999, -0.9994, -0.9960,  0.4689,  0.9863,  1.0000, -0.9732,  0.8855,\n",
       "          1.0000, -0.3671,  0.6653,  1.0000, -0.9800,  0.7309, -1.0000,  0.9405,\n",
       "          1.0000,  0.9940, -0.9662,  0.8792, -0.9890, -0.9998, -0.7537,  0.9995,\n",
       "         -0.6102,  0.9755,  0.9518, -0.9987, -1.0000,  0.9972, -0.9992, -0.7395,\n",
       "         -0.9595,  0.9998, -0.8678, -0.9898, -0.6613, -0.2772, -0.9999, -0.9999,\n",
       "          0.9941,  0.9990,  0.7310, -0.9995,  0.9999, -0.0472, -1.0000, -0.9627,\n",
       "         -1.0000, -0.6227, -0.9691,  0.9995, -0.9058,  0.2548,  0.9985, -1.0000,\n",
       "          0.9553, -0.9995, -0.9371,  0.9203,  0.9996,  0.9999,  0.9998, -0.6777,\n",
       "          0.9998,  1.0000,  0.9013,  0.9927, -0.9991,  0.9978,  0.9374, -0.6713,\n",
       "         -0.1837, -0.9147,  1.0000,  0.9981,  0.9565, -0.2769,  0.9550, -0.9988,\n",
       "          0.9999, -0.9999,  0.9991, -1.0000, -0.4198,  0.9827,  0.2440,  1.0000,\n",
       "         -0.2154,  0.9948, -0.8965, -0.9996,  0.4989, -0.7872,  0.9963, -0.9999,\n",
       "          0.0725, -0.9206,  0.4568, -0.6416, -1.0000,  0.9999, -0.9648,  1.0000,\n",
       "          0.9995, -0.9878, -0.9809, -0.9977, -0.5766, -0.9995, -0.3730,  0.9138,\n",
       "         -0.6070,  0.9991, -0.2683, -0.8986,  0.8922,  0.4851, -0.9725,  0.9972,\n",
       "         -0.4247,  0.9015,  0.6125,  0.9908,  0.9938,  0.8720, -0.8513,  0.9996,\n",
       "          0.4408,  0.0133,  0.9988,  0.6931,  0.3012, -0.9748, -1.0000, -0.1466,\n",
       "          0.9955, -0.8744, -0.9994, -0.4305, -0.9999,  0.9520, -0.1586, -0.8999,\n",
       "         -0.8274, -0.9999,  0.8430, -0.9715, -0.9999,  0.5183, -0.3143,  0.0951,\n",
       "         -0.9997,  0.3363,  0.9980, -0.7609,  0.4484, -0.3863, -0.9771,  0.9594,\n",
       "         -0.9318,  0.9901,  0.7576,  1.0000,  0.9843, -0.9732, -0.9316,  1.0000,\n",
       "         -0.5678, -1.0000,  0.7532, -0.9848, -0.1641,  0.9998, -0.9978,  0.7988,\n",
       "          1.0000, -0.6625,  1.0000, -0.0773, -0.9988, -0.9921,  1.0000,  0.9935,\n",
       "          0.9996, -0.9971, -0.8947,  0.3110, -0.6619, -1.0000, -0.9949, -0.2358,\n",
       "          0.9966,  0.9996, -0.0693, -0.9993, -0.6575, -0.9996,  1.0000, -0.4840,\n",
       "          0.9928,  0.9853, -0.5160,  0.3161,  0.4658, -0.1171, -0.9996,  0.1754,\n",
       "         -0.9939, -0.9919, -0.9999,  0.8289, -0.9990, -1.0000,  0.6392,  1.0000,\n",
       "         -0.3936, -0.9956,  0.9999,  0.9245, -0.8293,  0.6220,  0.9644, -1.0000,\n",
       "          1.0000, -0.9964,  0.9159, -0.9078, -0.9994, -0.4968,  0.9656,  0.9982,\n",
       "         -0.9980,  0.0202, -0.9921,  0.4786, -0.7622,  0.9150, -0.9011,  0.8037,\n",
       "         -0.9955, -0.7009,  0.6870, -0.8604, -0.9997,  0.9714,  1.0000, -0.1210,\n",
       "          1.0000,  0.9362,  1.0000, -0.9632, -0.9958,  0.9989, -0.4412, -0.9151,\n",
       "          0.6980,  0.4234,  0.8317,  0.2162, -0.8097, -0.9998,  0.9690, -0.2007,\n",
       "         -0.3896,  0.4418, -0.7819, -0.5045,  0.9948, -0.9885,  0.9949, -0.9996,\n",
       "         -0.3643,  0.9889,  0.9999,  0.9994,  0.7860, -0.8258,  0.9943, -0.9999,\n",
       "          0.9995, -0.9997,  0.9994, -0.7108,  0.4997, -0.9895, -0.9954,  1.0000,\n",
       "          0.9578,  0.9785,  0.9998, -0.7907,  0.9792, -0.7850,  0.9315,  0.9971,\n",
       "          0.8174,  0.9999, -0.9998, -0.9948,  0.1045, -0.9958, -0.9992, -1.0000,\n",
       "         -0.0878, -0.9993, -0.9113, -0.2522, -0.6346,  0.8421, -0.8795, -0.9602,\n",
       "          0.3377,  0.8545,  0.8049,  0.4718,  0.7568, -0.9983, -0.9723, -1.0000,\n",
       "         -0.9997, -0.8861,  0.9884, -0.9930,  0.9999, -0.9999,  0.2049, -0.6344,\n",
       "         -0.6671, -0.1085,  0.9634, -0.9836,  0.9979,  0.9998,  1.0000,  0.9984,\n",
       "          0.9415, -0.9748, -0.9994, -0.9708, -0.9911, -1.0000, -0.9984,  0.3286,\n",
       "         -0.3896, -1.0000,  0.1446,  0.9892,  1.0000,  0.9957, -0.9993, -0.7495,\n",
       "         -0.9999, -0.9933,  0.9999, -0.5375, -0.9997,  0.5888, -0.6150,  0.9999,\n",
       "         -0.3102, -0.4743,  0.5980,  0.7953,  0.9668, -0.9999,  0.9512,  1.0000,\n",
       "          0.9523, -1.0000,  0.8210, -0.6230, -0.9999, -0.6311,  0.9655,  0.9986,\n",
       "         -0.9960, -0.1882, -0.9959,  0.8748,  0.9704,  0.9996,  0.9988,  0.9570,\n",
       "          0.9186,  0.5490,  0.3927,  0.9937,  0.5834, -0.9995,  0.9999, -0.7988,\n",
       "          0.6152, -1.0000,  0.9997,  0.6679,  0.9929,  0.1063, -0.6543, -0.8505,\n",
       "         -0.9404,  0.9994,  1.0000, -0.2400, -0.9311, -0.9995, -0.9998, -0.9975,\n",
       "         -0.7993, -0.9109, -0.9771, -0.9987, -0.5967,  0.1591,  1.0000,  0.9973,\n",
       "          0.9722, -0.9637, -0.9392,  0.9939,  0.2040,  0.9909, -0.8267, -1.0000,\n",
       "         -0.9902, -0.9987,  0.9727, -0.5980, -0.0336, -0.9599,  0.9212,  0.9755,\n",
       "         -0.9997, -0.7976, -0.2381,  0.6471,  1.0000, -0.9774,  0.9688, -0.9985,\n",
       "          0.6127,  0.9407,  0.9369,  0.9977, -0.4862, -0.0987, -0.9141,  0.5641,\n",
       "          0.6122,  0.9954, -0.8175,  0.9624,  0.9999, -0.9844,  0.9732,  0.3042,\n",
       "          0.9228,  0.9913,  0.9999,  0.8759,  0.9248,  0.7861,  0.9725,  0.9999,\n",
       "          0.2130,  0.7354,  0.4698, -0.9525, -0.4108,  0.8048,  0.9919,  0.6928,\n",
       "         -0.0753, -0.9997,  0.9216,  0.9745,  1.0000, -0.8886,  0.9938, -0.0122,\n",
       "          0.6943,  0.7648,  0.8825,  0.6182,  0.6409,  0.9949,  1.0000, -0.9999,\n",
       "         -0.9963, -1.0000,  1.0000,  0.9982, -0.5378, -1.0000,  0.9998, -0.6722,\n",
       "         -0.6174,  0.9985, -0.6556, -0.9168,  0.5965, -0.9993,  0.4630,  0.1750,\n",
       "          0.9169, -0.2260,  0.9998, -0.9981,  0.4662,  1.0000,  0.2171,  0.9453,\n",
       "          0.5266, -0.9939,  0.9521, -0.9710, -0.9993, -0.5853,  0.9809,  0.9991,\n",
       "         -0.4306,  0.6146,  0.9924, -0.9468,  0.9999, -0.9998,  0.9532, -0.9835,\n",
       "          0.9998, -0.9893, -0.9996, -0.8455,  0.5925,  0.8769, -0.6102,  0.9999,\n",
       "          0.4429, -0.5567, -0.0339, -0.8934, -0.9991, -0.9935,  0.4555, -0.9922,\n",
       "          0.5980, -0.4123, -0.2262, -0.8286, -0.9830,  0.9997, -0.3822, -0.9899,\n",
       "          0.9999, -0.4126, -1.0000,  0.6576, -0.9922,  0.2383,  0.9867,  0.9662,\n",
       "          0.3407, -0.9999,  0.3415,  0.9877, -1.0000, -0.1630, -0.7957, -0.9356,\n",
       "          0.7237,  0.9657,  0.3847, -0.2224,  0.8774,  0.3389,  0.9622, -0.9330,\n",
       "         -0.7558, -0.9856, -0.9479, -0.9778, -0.9999, -0.9960, -0.9999,  1.0000,\n",
       "          0.8671,  0.9970, -0.8095, -0.0351,  0.8687,  0.9985, -0.9997, -0.9662,\n",
       "          0.1874,  0.9579, -0.8180, -0.9994,  0.4635, -1.0000, -0.3336,  0.4073,\n",
       "         -0.7958, -0.9341,  1.0000,  0.9989, -0.9995, -0.9827, -1.0000, -0.9834,\n",
       "          0.9999,  0.9969,  0.9996, -0.4878, -0.9366,  0.9908, -0.8523,  0.0658,\n",
       "         -0.9997, -0.9998, -0.9425,  0.8485, -0.9966, -0.9901,  0.9995,  0.9952,\n",
       "          0.3530, -1.0000, -0.8928,  0.9844,  0.9999,  1.0000,  0.9627,  0.9999,\n",
       "         -0.9921,  0.9988, -0.9975,  1.0000, -0.9999,  0.9999,  1.0000,  0.9800,\n",
       "          0.9986, -0.9986,  0.9418, -0.5205, -0.7158,  0.9396, -0.8234, -0.9936,\n",
       "         -0.9125,  0.9989, -0.9463,  1.0000,  0.4804,  0.8156,  0.9211, -0.5771,\n",
       "          0.8759,  0.4221, -1.0000,  0.5942,  0.9494,  0.9993,  1.0000,  0.9816,\n",
       "          0.9833, -0.9926, -0.9996, -0.0178, -0.8606, -0.4344, -0.9835,  0.9998,\n",
       "          1.0000, -0.9932, -0.0277,  0.8343,  0.6281,  0.9845,  0.9959, -0.9148,\n",
       "          0.3550,  0.4875,  0.9997, -1.0000,  0.9281, -0.9332, -0.7559,  0.9723,\n",
       "         -0.9478,  0.9999, -0.9990,  1.0000, -0.5291, -0.8135,  0.9999,  0.9944,\n",
       "         -0.9938,  0.9999,  0.7825, -0.9928, -0.7270, -0.9991, -0.9993,  0.2090]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "q = Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 101, 3193, 7953, 1962, 6629,  889, 1557,  511, 4495, 3189, 2571, 3556,\n",
      "         100, 1962, 2533, 2345,  679, 1914, 1568,  102]), tensor([ 101, 6929, 7938, 3193, 8013, 8013, 2769, 4638, 5440, 6275, 1453, 6917,\n",
      "        3760, 3300, 7274, 1993, 1450, 8013,  100,  679, 3632,  791, 2399, 8024,\n",
      "        2769, 4634, 4412,  872, 3680, 3613, 6963, 1962, 3193,  102]), tensor([ 101,  791, 1921,  679, 4761, 6887, 5543,  678, 7938, 8024,  679, 6882,\n",
      "        1921, 1348, 1107,  749, 5018,  671, 1842, 7434,  679, 3221, 7434, 5709,\n",
      "        4638, 3564, 2094, 8024, 3800, 2692, 7344, 2207, 1102, 7441, 1521,  102]), tensor([ 101, 2207, 1520, 1520, 3136, 3136, 2769,  100, 6241, 1001, 6716, 3136,\n",
      "         102])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2735],\n",
       "        [-0.2923],\n",
       "        [-0.0200],\n",
       "        [ 0.3614]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(prev_utterance = i['prev_utterance'], response = i['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_utterance': tensor([[ 101, 3193, 7953,  ...,   -1,   -1,   -1],\n",
       "         [ 101, 6929, 7938,  ...,   -1,   -1,   -1],\n",
       "         [ 101,  791, 1921,  ...,   -1,   -1,   -1],\n",
       "         [ 101, 2207, 1520,  ...,   -1,   -1,   -1]]),\n",
       " 'response': tensor([[1962, 2533, 2345,  ...,   -1,   -1,   -1],\n",
       "         [ 679, 3632,  791,  ...,   -1,   -1,   -1],\n",
       "         [5018,  671, 1842,  ...,   -1,   -1,   -1],\n",
       "         [6241, 1001, 6716,  ...,   -1,   -1,   -1]])}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = BertTokenizer(vocab_file = './vocab_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101, 100, 100, 102]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.encode([\"我我\", \"我我\"], return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(torch.tensor([[2, 3, 4]]))['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.pooler.dense.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, data_dir = '../LCCC-base', q_learning_rate = 1e-4, gpt_learning_rate = 1e-4, dqn_discounter = 0.95, dqn_alpha = 0.98):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set our init args as class attributes\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "        self.q_learning_rate = q_learning_rate\n",
    "        self.gpt_learning_rate = gpt_learning_rate\n",
    "        self.dqn_discounter = dqn_discounter\n",
    "        self.dqn_alpha = dqn_alpha\n",
    "\n",
    "        # Hardcode some dataset specific attributes\n",
    "        self.num_classes = 10\n",
    "        self.dims = (1, 28, 28)\n",
    "        channels, width, height = self.dims\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define PyTorch model\n",
    "        self.Q_a = \n",
    "\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_accuracy.update(preds, y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GPT2DataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = DataLoader(g, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 3193, 7953, 1962, 6629,  889, 1557,  511, 4495, 3189, 2571, 3556,\n",
       "         102,    1,  101, 6929, 7938, 3193, 8013, 8013, 2769, 4638, 5440, 6275,\n",
       "        1453, 6917, 3760, 3300, 7274, 1993, 1450, 8013,  102,    1,  101,  791,\n",
       "        1921,  679, 4761, 6887, 5543,  678, 7938, 8024,  679, 6882, 1921, 1348,\n",
       "        1107,  749,  102,    0,  101, 2207, 1520, 1520, 3136, 3136, 2769,  102,\n",
       "           1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i['prev_utterance'][i['prev_utterance'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2769, 2902,  872,  102])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.tokenizer.encode('我按你', return_tensors = 'pt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prev_utterance': tensor([[ 101,  872, 1343,  ...,   -1,   -1,   -1],\n",
      "        [ 101, 2769, 4500,  ...,   -1,   -1,   -1],\n",
      "        [ 101, 1490, 1490,  ...,   -1,   -1,   -1],\n",
      "        [ 101, 6250, 2533,  ...,   -1,   -1,   -1]]), 'response': tensor([[ 6887,  3624,  8013,  ...,    -1,    -1,    -1],\n",
      "        [10235,  8168,  3193,  ...,    -1,    -1,    -1],\n",
      "        [ 2402,  2130,  6857,  ...,    -1,    -1,    -1],\n",
      "        [  807,  6134,   749,  ...,    -1,    -1,    -1]])}\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "sample = []\n",
    "for i in h:\n",
    "    if counter == 1:\n",
    "        break\n",
    "    counter += 1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m h:\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(i)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:718\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 718\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    719\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_index\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 671\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampler_iter)\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/sampler.py:253\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m batch \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m    252\u001b[0m idx_in_batch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 253\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler:\n\u001b[1;32m    254\u001b[0m     batch[idx_in_batch] \u001b[39m=\u001b[39m idx\n\u001b[1;32m    255\u001b[0m     idx_in_batch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/torch/utils/data/sampler.py:76\u001b[0m, in \u001b[0;36mSequentialSampler.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mint\u001b[39m]:\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "for i in h:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../LCCC-base/single_emo_T_test.json') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['期待趙麗穎[喜歡]', '期待']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train[52:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = []\n",
    "for i in sample:\n",
    "    state.append(Reward.sentence2id(i, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0]['response'] = state[1]['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reward = Reward(gpt = gpt2, question_mark_token = 8043, toxic_words = [], eos_token = 102, device = 'cpu', gpt_tokenizer = tokenizer, non_sense_response = [[6857, 3564, 2094, 1621,  102], [1962, 1595,  102]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prev_utterance': tensor([ 101, 2769,  791, 1921, 2682, 6313, 2145, 2786, 1391, 6917, 3760, 1391,\n",
       "          2768,  738,  102,    1]),\n",
       "  'response': tensor([6250, 2533, 2380, 4891, 4289, 8024, 1190, 1726,  889, 8024, 5168, 6640,\n",
       "           102])},\n",
       " {'prev_utterance': tensor([ 101,  671, 6662, 6624, 1962,  172,  102,    1]),\n",
       "  'response': tensor([6250, 2533, 2380, 4891, 4289, 8024, 1190, 1726,  889, 8024, 5168, 6640,\n",
       "           102])},\n",
       " {'prev_utterance': tensor([ 101, 3173, 2094, 5507, 2137, 3298,  976, 4525, 4994, 7427, 7797, 3221,\n",
       "          2130, 5401, 4638,  102,    1], device='mps:0'),\n",
       "  'response': tensor([1506, 1506, 1506, 1506, 1506, 1506, 1506, 3221, 8013, 7427, 7797, 3297,\n",
       "          2130, 5401, 8013,  102], device='mps:0')},\n",
       " {'prev_utterance': tensor([ 101, 2769,  679, 1962, 8024, 5439, 2094, 6206, 4717,  749, 8024,  872,\n",
       "           738, 2571, 4717, 1416,  102,    1], device='mps:0'),\n",
       "  'response': tensor([ 872, 4412, 1762, 6917, 1377,  809, 4717, 8013, 8013,  102],\n",
       "         device='mps:0')},\n",
       " {'prev_utterance': tensor([ 101,  872, 3300, 3760, 3300, 1430,  748,  749, 7531, 7773,  102,    3],\n",
       "         device='mps:0'),\n",
       "  'response': tensor([1762, 7591,  704,  818, 2595, 4638, 7606, 5659,  102], device='mps:0')},\n",
       " {'prev_utterance': tensor([ 101, 2769, 6221, 2533, 6740, 3299, 1469, 3717, 3253,  738, 2923, 1962,\n",
       "          4638, 1217,  677, 1416, 8024, 7426, 4197, 2769, 2832,  749, 1476, 1565,\n",
       "          6486, 8024,  679, 6882, 3297, 2527, 6857, 1368, 6282, 5811, 1399, 2523,\n",
       "          3265,  749,  102,    1], device='mps:0'),\n",
       "  'response': tensor([2769, 2180, 1377, 1373, 7344, 4888, 1849, 6963,  679, 6206, 1373, 1476,\n",
       "          1565, 6486,  102], device='mps:0')},\n",
       " {'prev_utterance': tensor([ 101, 3312, 3300, 7938, 8043,  679, 3221, 3300, 6929,  943,  784, 7938,\n",
       "          4124,  889, 5865,  102,    1], device='mps:0'),\n",
       "  'response': tensor([7427, 3299, 4124, 8043, 1920,  765, 4124, 8043, 7427, 3299, 4124, 6206,\n",
       "          1762, 7770, 5993, 2798, 5543, 4692, 1168, 7427, 3299, 2479, 1798, 8024,\n",
       "          1920,  765, 4124,  156, 2399, 3760, 1343, 8024,  679, 4761, 4412, 1762,\n",
       "          3221, 2582, 7938, 3564,  511, 2669, 2336, 6656, 4403, 3862, 2345,  679,\n",
       "          1914, 8024, 4412, 1762, 3760, 3300, 4403, 3862,  746, 3912, 8024,  857,\n",
       "          5865, 5653, 3302, 8024,  852, 3221, 3760, 3300,  784, 7938, 1377,  809,\n",
       "          6879, 4381, 4638,  102], device='mps:0')},\n",
       " {'prev_utterance': tensor([ 101,  872, 1962, 8024, 6857, 3221, 2582, 7938, 1358, 1003, 4638, 1557,\n",
       "          8043, 1962, 1962,  828, 2622, 8024, 4867,  872, 3193,  763, 1962, 6629,\n",
       "           889, 1521, 8080,  102,    1], device='mps:0'),\n",
       "  'response': tensor([3559, 3461,  677, 3035,  678,  889, 4638, 8024, 5023, 3867, 5584,  749,\n",
       "          6206,  976, 2797, 6123, 8024, 6342, 6342, 7302, 2552,  102],\n",
       "         device='mps:0')}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3175, 0.4140, 0.3186, 0.3201, 0.3121, 0.2846, 0.4882, 0.3271])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3113126c51a710267751267ce303dd1adb7b26752a8847128aaa526c69b17c1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
